General architecture
In this section, we’ll go over the general architecture of the Transformer model.
Introduction
The model is primarily composed of two blocks:

Encoder (left): The encoder receives an input and builds a representation of it (its features). 
This means that the model is optimized to acquire understanding from the input.

Decoder (right): The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. 
This means that the model is optimized for generating outputs.

Each of these parts can be used independently, depending on the task:

Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.
Decoder-only models: Good for generative tasks such as text generation.
Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.

Attention layers
A key feature of Transformer models is that they are built with special layers called attention layers.
This layer will tell the model to pay specific attention to certain words in the sentence you passed it 
(and more or less ignore the others) when dealing with the representation of each word.

The original architecture:
The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, 
while the decoder receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence 
(since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, 
works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently 
being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the 
inputs of the encoder to try to predict the fourth word.

To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words 
(if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict 
the fourth word, the attention layer will only have access to the words in positions 1 to 3.

The original Transformer architecture looked like this, with the encoder on the left and the decoder on the right:

Architecture of a Transformers models
Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the 
encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that 
put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.

The attention mask can also be used in the encoder/decoder to prevent the model from paying attention to some special words — for instance, the special padding 
word used to make all the inputs the same length when batching together sentences.
